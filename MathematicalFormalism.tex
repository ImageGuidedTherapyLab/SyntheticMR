%
%  This simple example illustrates how documents can be
%  split into smaller segments, each segment processed
%  by latex2html separately.  This document can be
%  processed through latex and latex2html with the
%  corresponding makefile.
%

\documentclass{article}         % Must use LaTeX 2e
\usepackage[plainpages=false, colorlinks=true, citecolor=black, filecolor=black, linkcolor=black, urlcolor=black]{hyperref}		
\usepackage[left=.75in,right=.75in,top=.75in,bottom=.75in]{geometry}
\usepackage{makeidx,color,boxedminipage}
\usepackage{graphicx,float}
\usepackage{amsmath,amsthm,amsfonts,amscd,amssymb} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	Some math support.					     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%	Theorem environments (these need the amsthm package)
%
%% \theoremstyle{plain} %% This is the default

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{ax}{Axiom}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

\theoremstyle{remark}
\newtheorem{rem}{Remark}[section]
\newtheorem*{notation}{Notation}
\newtheorem*{exrcs}{Exercise}
\newtheorem*{exmple}{Example}

%\numberwithin{equation}{section}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	Macros.							     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%	Here some macros that are needed in this document:

\newcommand{\motion}{\mathbf{\varphi}}
\newcommand{\hmotion}{\mbox{\boldmath $\hat{\varphi}$}}
\newcommand{\cauchy}{\mbox{\boldmath $\sigma$}}
\newcommand{\eqn}[1]{(\ref{#1})}
\newcommand{\hOmega}{\hat{\Omega}}
\newcommand{\homega}{\hat{\omega}}
\newcommand{\nphalf}{n+\frac{1}{2}}
\newcommand{\nmhalf}{n-\frac{1}{2}}
\newcommand{\kmhalf}{k-\frac{1}{2}}
\newcommand{\kphalf}{k+\frac{1}{2}}
\newcommand{\picdir}{pdffig/}

\include{symbols}
\title{Advanced Applications of Synthetic MR and MAGiC}
\author{}
\begin{document}                % The start of the document
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Statement}\label{sec:prob_statement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Consider a magnetization signal $M_{TD}$ that is defined as
as function of 
\textbf{acquisition parameters} $\kk=\{T_R,T_D,\theta,T_E,\alpha\}$ 
{\color{red}(@kenphwang are we considering $\alpha$ a control parameter?)}
and \textbf{tissue properties} $\p \equiv \{T_1,T_2,M_0\}$. 

\begin{figure}[h] 
\centering
\begin{tabular}{ccc}
\includegraphics[width=.7\textwidth]{\picdir/PulseSequence.png} & 
\includegraphics[width=.29\textwidth]{\picdir/NeuroBasis.png} \\
(a) & (b) \\
\end{tabular}
\caption{ 
(a) Synthetic MR Pulse sequence. (b) Neuroimaging basis.
}\label{fig:Pulsesequence}
\end{figure}

\begin{equation}\label{mtd}
M_{TD}(\kk,\p,\x)=\left(M_0(\x)\frac{1-(1-\cos \theta)e^{-\frac{T_D}{T_1(\x)}}-\cos \theta e^{-\frac{T_D}{T_1(\x)}}}{1-\cos \theta e^{-\frac{T_R}{T_1(\x)}}cos \alpha}\right) e^{-\frac{T_E}{T_2(\x)}}
\end{equation}
Here, $M_0$ is the unsaturated magnetization, $\theta$ represents the \textit{saturation} flip
angle, and $T_R$ and $T_E$ denote repetition time and echo time, respectively.
Parameters $T_1$ and $T_2$ represents relaxation times, and $\alpha$ is the
\textit{local} excitation flip angle. 
In general, excitation pulse $\alpha$ is a function of flip angle, i.e.
$\alpha=\alpha(\theta)$
{\color{red}(@kenphwang why is this?)}.
Note that the unsaturated magnetization $M_0$, along with
relaxation times $T_1$ and $T_2$, are a function of spatial coordination $\x$.
Basis functions $\phi_i$ represent the neuro anatomy. For completeness,
consider
$\phi_1 = \phi_\text{gm}$,
$\phi_2 = \phi_\text{wm}$,
$\phi_3 = \phi_\text{csf}$,
$\phi_4 = \phi_\text{tumor}$ as a simplified set 
of the regions illustrated in Figure~\ref{fig:Pulsesequence}(b).
\[
T_1(\xbf)=\sum_{i=1}^{N=4} T1_i \phi_i(\xbf)
\qquad
T_2(\xbf)=\sum_{i=1}^{N=4} T2_i \phi_i(\xbf)
\qquad
M_0(\xbf)=\sum_{i=1}^{N=4} M0_i \phi_i(\xbf)
\]
	
	
\[
\bigcup_{i=1}^{N=4}\Omega_i=\Omega  \qquad  \Omega_n\cap\Omega_m=\varnothing
\qquad 
\phi_i(\xbf)=\left\{ \begin{split}
			1 & \quad x\in\Omega_i \\
			0 & \quad \text{otherwise}
                     \end{split} \right. 
\]


Assume that the signal model  for $M_{TD}$ \eqn{mtd} is our measurment model in \textbf{image space}
and is polluted with a white noise $\nu$ (with mean zero and variance $\R$). Hence, \eq{mtd} can be written as:
\begin{equation}\label{mtdobs}
z(\kk,\p)=\underbrace{M_{TD}(\kk,\p,\x)}_{h(\kk,\p)}+\nu
\end{equation}


Note that the observation $z$ is a function of control parameters $\kk$ and
parameters of interest $\p$.  The ultimate goal is to provide accurate estimate
of the parameters $\p$, given some measurements $z$. 
Precise estimation of parameters $\p$ crucially depends on the values of
control parameters $\kk=\{T_R,T_D,\theta,T_E\}$.  In other words, to ensure
performance of the estimation algorithm, one needs to select the control
parameters $\kk=\{T_R,T_D,\theta,T_E\}$ such that the observation $z$ provides
useful information about the parameters $\p$. This is achieved my maximizing
the mutual information between the control parameters $\kk$ and parameters of
interest $\p$. 
Within this framework we will consider the tissue properties to be normally distributed
Gaussian parameters.
\[
  \text{T1}_\text{WM}    =  \mathcal{N}(100ms, 20ms)  \qquad
  \text{T1}_\text{GM}    =  \mathcal{N}(120ms, 20ms)  \qquad
  \text{T1}_\text{CSF}   =  \mathcal{N}(320ms, 20ms)  \qquad
  \text{T1}_\text{Tumor} =  \mathcal{N}(300ms, 20ms)
\]
\[
  \text{T2}_\text{WM}    =  \mathcal{N}(100ms, 20ms)  \qquad
  \text{T2}_\text{GM}    =  \mathcal{N}(120ms, 20ms)  \qquad
  \text{T2}_\text{CSF}   =  \mathcal{N}(320ms, 20ms)  \qquad
  \text{T2}_\text{Tumor} =  \mathcal{N}(300ms, 20ms)
\]
\[
  \text{M0}_\text{WM}    =  \mathcal{N}(100??, 20??)  \qquad
  \text{M0}_\text{GM}    =  \mathcal{N}(120??, 20??)  \qquad
  \text{M0}_\text{CSF}   =  \mathcal{N}(320??, 20??)  \qquad
  \text{M0}_\text{Tumor} =  \mathcal{N}(300??, 20??)
\]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{WIP - Optimal Experimental Design}\label{oed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As discussed before, performance of estimation process crucially depends on the value of control parameters $\kk$. Hence, it is important to develop mathematical tools to identify the control parameters $\kk$ such that they provide the best observation data for accurate estimation of parameter $\p$. This is equivalent with maximizing the mutual information between the observation data and parameters $\p$. Based on information theory, mutual information is defined as the reduction of uncertainty in one parameter due to knowledge of the other parameter.

\begin{equation} \label{mi}
I(\p;z)=\int_z\int_p p(\p,z)\ln\left(\frac{p(\p,z)}{p(\p)p(z)}\right)d\p dz
\end{equation}

We make use of Bayes theorem to simplify the above equation. By substituting $p(\p,z)$ with $p(z|\p)p(\p)$, \eq{mi} can be written as:

\begin{equation} \label{mi2}
I(\p;z)=\int_z\int_p p(z|\p)p(\p)\ln\left(\frac{p(z|\p)p(\p)}{p(\p)p(z)}\right)d\p dz
\end{equation}
Or,
\begin{equation} \label{mi3}
I(\p;z)=\int_z\int_p p(z|\p)p(\p)\ln\left[p(z|\p)\right]d\p dz - \int_z p(z) \ln p(z)dz
\end{equation}



Note that due to dependence of observation data $z$ on control parameters, the mutual information $I(\p;z)$ is a function of control parameter $\kk$. In order to maximize the reduction of uncertainty in parameter estimate (i.e. to have the most confident estimates of the parameter $\p$), one can simply maximize the mutual information between the observation data and parameters of interest:

\begin{equation}\label{mimax}
\max_{\kk} I(\p;z)
\end{equation}

The above maximization results in \textit{optimal} values of control parameter $\kk$ for accurate estimation of parameter $\p$. Note that in \eq{mi3}, $p(z|\p)$ is defined as a Gaussian distribution with mean $h(\kk,\p)$ and variance $\R$. As well, $p(\p)$ denotes the prior distribution of parameter $\p$, which for the ease of calculations, is considered to be a Gaussian distribution with some prior mean $\hat{\p}^-$ and prior covariance $\Sigma^-$, i.e. $p(\p)\sim \mathcal{N}(\hat{p}^-,\Sigma^-)$. Method of quadrature points can be used to evaluate \eq{mi3}. 

We emphasize here that the mutual information will be the same on different pixels with the same tissue types. This is due to the similarities in statistics of $\p$ between the two different pixels with the same tissue properties. In other words, whenever two different pixels have the same tissue properties, then the distribution of parameter $\p$, denoted by $p(\p)$, is the same and so is the value of mutual information. Hence, there is no need to evaluate the mutual information for each pixel in a region with the same tissue type. 

On the other hand, in a case that the tissue properties for each pixel are different from the other, then the mutual information needs to be evaluated for each and every pixel of interest.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{WIP - Model Data Fusion}\label{da}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
After finding the optimal values of the control parameter $\kk$, we can proceed and perform the model-data fusion to get a better understanding about the uncertainties involved in parameters $\p$.
The fusion of observational data with mathematical model predictions promises to provide greater understanding of physical phenomenon than either approach alone can achieve. In here, a minimum variance framework is being used for model - data fusion. Based on minimum variance technique, posterior statistics of parameter $\p$ can be written as:

\begin{eqnarray}
\hat{\p}^+=\hat{\p}^-+\K[z-\underbrace{\Ex{h(\kk,\p)}{-}}_{h^-}]\label{minvarmean}\\
\Sigma^+=\Sigma^-+\K\Sigma_{hh}\K^T\label{minvarvar}
\end{eqnarray}
where, %$z\equiv \tilde{M}_{TD}$ and 
the gain matrix $K$ is given by
\begin{eqnarray}\label{kgain}
\K=\Sigma_{\p z}\left(\Sigma_{hh}^-+\R\right)^{-1}
\end{eqnarray}

Here, $\hat{\p}^-$ and $\hat{p}^+$ represent prior and posterior values of the mean for parameter vector $\p$, respectively:
\begin{eqnarray}
\hat{\p}^-\equiv \Ex{\p}{-}=\int \p^- p(\p)d\p \label{Pprior}
\hat{\p}^+\equiv \Ex{\p}{+}=\int \p^+ p(\p)d\p \label{Pposterior}
\end{eqnarray}
where, $p(\p)$ denotes the probability density function of parameter $\p$. Similarly, the prior and posterior covariance matrices $\Sigma^{-}$ and $\sigma^+$ can be written as:

\begin{eqnarray}
\Sigma^-\equiv\Ex{(\p-\hat{\p}^-)(\p-\hat{\p}^-)^T}{}\\
\Sigma^+\equiv\Ex{(\p-\hat{\p}^+)(\p-\hat{\p}^+)^T}{}
\end{eqnarray}

The matrices $\Sigma_{\p z}$ amd $\Sigma_{hh}$ are defined as:

\begin{eqnarray}
\Sigma_{\p z} \equiv\Ex{(\p-\hat{\p})(h-\hat{h}^-)^T}{}\\
\Sigma_{hh} \equiv\Ex{(h-\hat{h}^-)(h-\hat{h}^-)^T}{}
\end{eqnarray}

\eq{minvarmean} along with \eq{minvarvar} provide posterior mean and covariance of parameter $\p$ given observation data $\tilde{z}$ and model predictions $h(\kk,\p)$.
We emphasize here that the optimal values of $\kk$, obtained from \eq{mimax}, are used in \eq{minvarmean}.

\section{WIP - Overall Picture}
The following diagram illustrates the general work-flow of the process:
\begin{figure}[h!!]
\center
\includegraphics[trim = 1.5cm 3cm 8cm 0.5cm, clip,width=5in,height=4.5in]{\picdir/flowchart.pdf}
\caption{Schematic view of the estimation process}
\end{figure}

\section{WIP - T1, T2, M0 Reconstruction}
\paragraph{Given}
the image space date for  multiple  acquistion parameters 
$\{ M_{TD}(\kk_1),M_{TD}(\kk_2), M_{TD}(\kk_2),... \}$,  \\
 $\kk_i=\{T_{R_i},T_{D_i},\theta_i,T_{E_i},\alpha_i\}$ 
{\color{red}(@kenphwang 2 delay times and 4 echoes correct?)},

The reconstruction algorithms for T1, T2, M0 is as follows:
\begin{itemize}
\item {\color{red}(@kenphwang can we get the current code for this recon.)}
\item {\color{red}(@kenphwang Can  you provide an example data set ? is this real/imaginary data? or magnitude only? is $\alpha$, $\theta$ fixed?)}
\item 
\end{itemize}


%\begin{figure}[htb!]
%\begin{tabular}{ccc}
%\hspace{-0.25in}\subfigure{\includegraphics[width=2.5in]{pics/t2.eps}}&
%\hspace{-0.25in}\subfigure{\includegraphics[width=2.5in]{pics/t2meas.eps}}&
%\hspace{-0.25in}\subfigure{\includegraphics[width=2.5in]{pics/err2.eps}}
%\end{tabular}
%\caption{Temperature map over slice 2 a) Reconstructed temperature, b) observed temperature, c) the error between the reconstruction and measurement.}\label{t2}
%\end{figure}
% % t2 Fig
 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ WIP - Physics Model}\label{PhysicsModel}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The steady-state magnetization $M_{TD}$ at a specific delay time $T_D$ can be found as a function of flip angle $\theta$, repetition time $T_R$, excitation pulse $\alpha$, and relaxation time  $T_1$:
\[
  M_{TD}  =  M_0\frac{1-(1-\cos(\theta))e^{-\frac{T_D}{T_1}}-\cos(\theta)e^{-\frac{T_R}{T_1}}}{1-\cos(\theta)e^{-\frac{T_R}{T_1}}\cos(\alpha)}
\]
where, $M_0$ is the unsatuarated magnetization.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ WIP - Mathematical Framework}\label{GeneralMathFramework}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The underlying philosophy and assumptions within our approach is that the physics 
models are 1st order accurate or within 70-80\% of the needed accuracy and the error is
adequate within the assumed Gaussian noise.
Gaussian distributions provide analytical representations of the random
variables of interest (ie T1, T2) within the Bayesian setting and 
provide a crux for understanding. In particular, we say that a random
variable $\eta$ belongs to a multi-variate normal distribution 
of mean $\mu \in \mathbb{R}^n $ and covariance $\Sigma \in \mathbb{R}^{n \times n}$
\[
     \eta \sim \mathcal{N}(\mu,\Sigma)  
    \Rightarrow
      p(\eta)  = \frac{1}{2 \; \pi \; \det{\Sigma}} \exp\left( - \frac{1}{2} \| \mu - \eta\|^2_{\Sigma}\right)
\]


\begin{enumerate}
  \item Our data acquistion model, $\mathcal{G}(\vec{k},\theta): \mathbb{R}^a
\times \mathbb{R}^m \rightarrow \mathbb{R}^n $,
maps deterministic acquisition
parameters, $\vec{k} \in \mathbb{R}^a$, and uncertain parameters, $\theta \in \mathbb{R}^m$
to observables, $\vec{z} \in \mathbb{R}^n$ ( or $\vec{z} \in \mathbb{C}^n$).
Explicitly, we will assume that the
measurement models are corrupted by zero mean white noise noise of a
\textbf{known} covariance matrix, $\Sigma_z \in \mathbb{R}^{n \times n}$ 
\begin{equation}
\label{sensormodelstructure}
\begin{split}
  \vec{z} & = \mathcal{G}(\vec{k};\theta) + \eta   \qquad   \eta \sim \mathcal{N}(0,\Sigma_z)
      \\
  \vec{k} & =  \text{(TE, TR, etc)}
      \\
  \theta &  =  \text{(T1, T2, etc)}
     \end{split}
\end{equation}
$\eta$ may be interpreted as the measurement noise or the acquisition noise
in the sensor model. For a deterministic measurement model $\mathcal{G}$,
the conditional probablity distribution has an explicit analytical form
and may be written as a  \textbf{known} Gaussian
distribution. 
  \[ 
      p(\vec{z}|\theta)   =  \mathcal{N}(\mathcal{G}(\vec{k};\theta),\Sigma_z)  
  \]
  \item Additional \textbf{known} information is the prior probability
distributions for the model parameters, $p(\theta)$.  For simplicity,
    assume that Prior parameters are Gaussian distributed of 
   \textbf{known} mean, $\hat{\theta}$ and covariance, $\Sigma_\theta$
   \[
      \theta \sim \mathcal{N} (\hat{\theta}, \Sigma_\theta)
   \]
  \item Bayes theorem is fundamental to the approach.
The probability of the measurements $p(z)$ must be interprited in terms of the
known information. The probability of the measurements may be derived from
the marginalization of the joint probability and has the interpretation as
the projection of the joint probability onto the measurement axis.
\[
  p(z) = \int_\theta p(\theta,z)  \; d\theta 
       = \int_\theta p(z|\theta) \; p(\theta)\; d\theta 
\]
  \item  The concept of informational entropy~\cite{Madankan15}, $H(Z)$,
provides a mathematically rigorous framework to look for measurement acquisition
parameters, $\vec{k}$, with the high information content of the reconstruction.
Given a probability space 
$(\Omega, \mathcal{F},p)$ (probability maps from the
sigma-algebra of possible events $p:\mathcal{F}\rightarrow [0,1]$
sigma-algebra, $\mathcal{F}$, defined on set of `outcomes' $\Omega$
\cite{durrett2010probability}),
we will define information of an event  as
proportional to the inverse probability.
\[
\text{information} \equiv  \frac{1}{p(z)}
\]
Intuitively, when a low probability event occurs this provides high
information.
The informational entropy is an \textit{average}
of the information content for a sigma algebra of events $\mathcal{F}$
\[
H(Z) = \int_Z  p(z) \ln\frac{1}{p(z)} \; dz
\qquad
  p(z) = \int_\theta p(z|\theta) \; p(\theta)\; d\theta 
\]
Hence this entropy measure is an average of the information content
for a given set of events, $\mathcal{F}$, and is proportional to the
variance or uncertainty in which the set of events occur.
This agrees with thermodynamic entropy;
if the information containing events are completely spread out such as in a
uniform distribution, the entropy is maxmized.
The entropy
is zero for a probability distribution in which
only one event occurs. Zero information is gained when the same event
always occurs ($0 \ln\frac{1}{0} = 0$). 
Intuitively, we want to find acquisition parameters,
$\vec{k}$, for which the measurements are most uncertain
\[
\max_k H(Z)
  \quad \Leftrightarrow \quad
\min_k 
    \int_Z   dz \underbrace{\int_\theta  d\theta \; p(z|\theta) \; p(\theta)}_{p(z)}
                \underbrace{ln \left(\int_\theta  d\theta \; p(z|\theta) \; p(\theta)\right)}_{ln \; p(z)}
\]
Alternatively we may consider this entropy maximization problem as a
sensitivity analysis for the variance of the measurement $Z$, ie . 
$ \max_k H(Z) \approx  \max_k \text{Var}(Z) $
\[ \begin{split}
   \bar{Z} = \mathbb{E}[Z]  & = \int_Z   dz \; z
   \underbrace{\int_\theta  d\theta \; p(z|\theta) \; p(\theta)}_{p(z)}
  \\
   \mathbb{E}[ ( Z - \bar{Z} )^2 ]  & = \int_Z   dz \;(z - \bar{Z})^2 
   \underbrace{\int_\theta  d\theta \; p(z|\theta) \; p(\theta)}_{p(z)}
  \\
   &  \propto
    \int_Z   dz \;(z - \bar{z})^2 \int_\theta  d\theta 
 \exp\left( - \frac{1}{2} \|  z- \mathcal{G}(\vec{k},\theta)  \|^2_{\Sigma_z}\right)
 \exp\left( - \frac{1}{2} \|  \theta - \hat{\theta}  \|^2_{\Sigma_\theta}\right)
 \end{split}
\]
Probilistic integrals may be
computed from uncertainty quantification
techniques~\cite{fahrenholtz2013generalised}.

 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{WIP - Echo train length }\label{ModelFidelity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% http://mriquestions.com/fse-parameters.html
% http://mriquestions.com/what-is-fsetse.html

In conventional spin-echo imaging, two basic timing parameters are
required, repetition time (TR) and echo time (TE), Figure~\ref{fig:echotrain}(a).
Similar to
fast spin echo (FSE) imaging, 
the acquistion is setup to acquire multiple lines of k-space in a single TR.
In this situation,
TE is replaced by effective echo
time  and addition parameters are needed: 

\begin{itemize}
\item TE$_\text{eff} \equiv$ the time at which the central lines of k-space are being filled.
\item Number of echoes $\equiv$ called echo train length (ETL)
\item Time between echoes $\equiv$ called echo spacing (ESP) 
\end{itemize}



\begin{figure}[h] 
\begin{tabular}{ccc}
\includegraphics[width=.3\textwidth]{\picdir/7162303.png}
&
\includegraphics[width=.3\textwidth]{\picdir/5013564.png}
&
\includegraphics[width=.3\textwidth]{\picdir/3316708_orig.png}
\\
(a) & (b) & (c) \\
\end{tabular}
\caption{ 
(a)
}\label{fig:echotrain}
\end{figure}


{\color{red}
   TODO - need to update signal model for multiple read out lines
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{WIP - Inverse Problem Framework}\label{InverseProbFramework}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[
  \vec{z}  = \mathcal{G}(\theta) + \eta   \qquad   \eta \sim \mathcal{N}(0,\Sigma_z)
\]
\[
  p(z|\theta ) = \exp \left( \|\vec{z} -  \mathcal{G}(\theta)\|^2_{\Sigma_z} \right)
\]
\[
               d\left(\vec{z}, \mathcal{G}(\theta^*)\right) = 
   \min_{\theta \in \Omega} d\left(\vec{z}, \mathcal{G}(\theta)\right)
\qquad
\theta = \left(\mu_\text{CSF}, \mu_\text{GM}, \mu_\text{WM}, \mu_\text{Tumor} \right)
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\nocite{*}
\bibliographystyle{apalike}
\bibliography{references}

\appendix
\section{Bayes - An intuitive example}
Bayes theorem is fundamental to the approach and is immediately
follows from the definition of conditional probability
\[
\left.
\begin{split}
p(y|x)  \equiv  \frac{p(x,y)}{p(x)}  \\
p(x|y)  \equiv  \frac{p(x,y)}{p(y)} 
\end{split}
\right\}
\Rightarrow
p(y|x) p(x) = p(x,y) = p(x|y) p(y)
\Rightarrow
\hspace{-1in}
\begin{split}
p(y|x)  =\frac{ p(x|y) p(y) }{p(x) } \\
p(x|y)  =\frac{ p(y|x) p(x) }{p(y) } \\
\end{split}
\]

As a concrete example, consider the explicit two dimensional joint Gaussian
distribution as a medium for understanding. Here we have two random
variables $\mathbf{x}_1$ and $\mathbf{x}_2$ defined on the same probability
space, $\Omega$.
\[
\mathbf{x}_i: \Omega \rightarrow \mathbb{R}
\qquad
P\left( \left\{ \omega: 
\mathbf{x}_i (\omega) \in A
 \right\}\right)
=
\int_A p(\eta_i) d\eta_i
\]
Intuitively, if we are \textbf{given} the joint distribution,
$p(\eta_1,\eta_2)$, knowledge of the realization of one particular random
variable provides information on the realization of the second random
variable.
\[
      p(\eta_1,\eta_2)  = \frac{1}{2 \; \pi \; \sqrt{\det{\Sigma}}}
\exp\left( \frac{1}{2}
\begin{bmatrix}
\eta_1 - \mu_1 \\
\eta_2 - \mu_2 \\
\end{bmatrix}^\top
\underbrace{
\begin{bmatrix}
       \sigma_1^2        & r_{12} \sigma_1 \sigma_2 \\
r_{12} \sigma_1 \sigma_2 &          \sigma_2^2 \\
\end{bmatrix}
}_{\equiv \Sigma}
\begin{bmatrix}
\eta_1 - \mu_1 \\
\eta_2 - \mu_2 \\
\end{bmatrix}
\right)
\]

See \cite{maybeck1979stochastic} (Sec 3.10), characteristic functions 
are used to show that individual marginal
densities of joint  Gaussian random variable is also Gaussian.
\[
p(\eta_1) = 
\int_{\eta_2}
      p(\eta_1,\eta_2)
\;d\eta_2
 = \frac{1}{ \sqrt{2 \; \pi \; \sigma_2^2}} \exp\left( - \frac{(\eta_1 -
\mu_1)^2}{2 \sigma_1^2} \right)
\]

\[
p(\eta_2) = 
\int_{\eta_1}
      p(\eta_2,\eta_1)
\;d\eta_1
 = \frac{1}{ \sqrt{2 \; \pi \; \sigma_1^2}} \exp\left( - \frac{(\eta_2 -
\mu_2)^2}{2 \sigma_2^2} \right)
\]

Conditional probablity is \textit{defined} through the algegraic
reduction of the ratio of the joint and the marginal densities
\[
p(\eta_1|\eta_2) =  \frac{p(\eta_1,\eta_2)  }{p(\eta_2) }
 = \frac{1}{ \sqrt{2 \; \pi \; \sigma_{1|2}^2}}
    \exp\left( - \frac{(\eta_1 - \mu_{1|2})^2}{2 \sigma_{1|2}^2} \right)
 = 
\frac{p(\eta_1)}{p(\eta_2)}
   \frac{1}{ \sqrt{2 \; \pi \; \sigma_{2|1}^2}}
    \exp\left( - \frac{(\eta_2 - \mu_{2|1})^2}{2 \sigma_{2|1}^2} \right)
\]
\[
\mu_{1|2} =  \mu_1 - 
       \frac{r_{12} \sigma_1 \sigma_2 }{\sigma_2^2}
       (\eta_2 - \mu_2)
\qquad
\sigma_{1|2} = 
\sigma_1^2  - 
       \frac{(r_{12} \sigma_1 \sigma_2 )^2}{\sigma_2^2}
\]
\[
\mu_{2|1} =  \mu_2 - 
       \frac{r_{12} \sigma_1 \sigma_2 }{\sigma_1^2}
       (\eta_1 - \mu_1)
\qquad
\sigma_{2|1} = 
\sigma_2^2  - 
       \frac{(r_{12} \sigma_1 \sigma_2 )^2}{\sigma_1^2}
\]

\end{document}
